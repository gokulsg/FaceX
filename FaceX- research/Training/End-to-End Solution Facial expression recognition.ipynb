{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"End-to-End Solution Facial expression recognition.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1QOTE359xjHVcFtFP6SLeHEKzgRAdU4OW","authorship_tag":"ABX9TyPQtn3Tj3/ip1GQLHCR/BiC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mRaqs5djs8w_","colab_type":"text"},"source":["# **1. SEED**"]},{"cell_type":"code","metadata":{"id":"3ji-jJfWuD8z","colab_type":"code","colab":{}},"source":["# Seed value\n","seed_value= 0\n"," \n","# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n","import os\n","os.environ['PYTHONHASHSEED']=str(seed_value)\n"," \n","# 2. Set `python` built-in pseudo-random generator at a fixed value\n","import random\n","random.seed(seed_value)\n"," \n","# 3. Set `numpy` pseudo-random generator at a fixed value\n","import numpy as np\n","np.random.seed(seed_value)\n"," \n","# 4. Set `tensorflow` pseudo-random generator at a fixed value\n","import tensorflow as tf\n","tf.random.set_seed(seed_value)\n"," \n","# 5. Configure a new global `tensorflow` session\n","session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n","sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n","tf.compat.v1.keras.backend.set_session(sess)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CfXAALNus8ug","colab_type":"text"},"source":["# **2. Load packages**"]},{"cell_type":"code","metadata":{"id":"TQGnaA40uEk5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"8b5e09e0-4076-465e-9690-4a100039ec61","executionInfo":{"status":"ok","timestamp":1591423546234,"user_tz":-180,"elapsed":7042,"user":{"displayName":"Paul Damsa","photoUrl":"","userId":"14307149635844850697"}}},"source":["# import the necessary\n","from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import tensorflow.keras.backend as K\n","\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","import seaborn as sns\n","import pandas as pd\n","import datetime\n","import math\n","import time\n","import os\n","import csv"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"mZ6OtP6cs8s3","colab_type":"text"},"source":["# **3. Load Data**"]},{"cell_type":"code","metadata":{"id":"FqIw_qdjuFO2","colab_type":"code","colab":{}},"source":["# Load the data from the csv file.\n","# path = path to your csv file with FER2013 dataset\n","path = '/content/drive/My Drive/datasets/facial-expression/fer2013/new-fer2013-dataset-balanced.csv'\n","fer_dataset = pd.read_csv(path)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MCd2SzVVs8qg","colab_type":"text"},"source":["# **4. Data preprocessing**"]},{"cell_type":"code","metadata":{"id":"5lK40mq2uF3d","colab_type":"code","colab":{}},"source":["# Shuffle the dataset \n","fer_dataset = fer_dataset.sample(frac=1, random_state=seed_value).reset_index(drop=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCUDZDr2d-xD","colab_type":"code","colab":{}},"source":["# Function for normalizing the dataset\n","# input_set: dataset \n","# scaler: the scaler for normalize\n","# @return: dataset with normalized data\n","def normalizeSet(input_set, scaler):\n","  x = scaler.fit_transform(input_set)\n","  return x\n","\n","# The function for getting the pixels as a list \n","# input_set: dataset \n","# @return: an array of flattened images\n","def getPixels(input_set):\n","  pixels = input_set['pixels'].tolist()\n","  new_pixels = []\n","  for pixel_sequence in pixels:\n","    image = [int(pixel) for pixel in pixel_sequence.split(' ')]\n","    new_pixels.append(image)\n","  return np.asarray(new_pixels, dtype='float32')  \n","\n","# The function for reshpping the dataset\n","# cutsize: if True the image will be cutted\n","# input_set: dataset with only pixels (# the return of the getPixels(input_set))\n","def reshapeDataset(input_set, cutsize=False):\n","  new_input_set = []\n","  for image in input_set:\n","    # reshape the image in (48, 48) shape\n","    image = image.reshape((48, 48))\n","\n","    # add new axis for being in (48, 48, 1) shape\n","    image = image[:, :, np.newaxis]\n","\n","    # reshape in (48, 48, 3) shape by concatenating 3 times the image\n","    # image = np.concatenate((image, image, image), axis=2)\n","    new_input_set.append(image)\n","  return np.asarray(new_input_set, dtype='float32')\n","\n","# This function is going to help with normalizing and reshapping a set\n","def normalize_reshape_set(input_set, scaler, cutsize):\n","  images = getPixels(input_set)\n","  print('Done with pixels of images')\n","\n","  images = normalizeSet(images,scaler)\n","  print('Done normalizing the images')\n","\n","  reshapped_images = reshapeDataset(images,cutsize)\n","  print('Done reshaping the images')\n","\n","  return reshapped_images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-W6xfGyweHCC","colab_type":"code","colab":{}},"source":["# We want to preparing the data set for normalizing part\n","data_for_training = fer_dataset[fer_dataset['Usage'] == 'Training']\n","\n","data_for_validation = fer_dataset[fer_dataset['Usage'] == 'PublicTest']\n","data_for_testing = fer_dataset[fer_dataset['Usage'] == 'PrivateTest']\n","\n","# Print the shape of the sets\n","print('Train shape = ',data_for_training.shape)\n","print('Validation shape = ',data_for_validation.shape)\n","print('Test shape = ',data_for_testing.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X2nuieZteL5s","colab_type":"code","colab":{}},"source":["# Drop the column Usage from train, validation and test sets\n","data_for_training = data_for_training.drop(['Usage'],axis=1)\n","data_for_validation = data_for_validation.drop(['Usage'],axis=1)\n","data_for_testing = data_for_testing.drop(['Usage'],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rq_jNYtyePrC","colab_type":"code","colab":{}},"source":["# Split the data in X and y format\n","\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","X_train = normalize_reshape_set(data_for_training, scaler, cutsize = False)\n","X_val = normalize_reshape_set(data_for_validation, scaler, cutsize = True)\n","X_test = normalize_reshape_set(data_for_testing, scaler, cutsize = True)\n","\n","y_train = np.asarray(data_for_training['emotion'], dtype='int32')\n","y_val = np.asarray(data_for_validation['emotion'], dtype='int32')\n","y_test = np.asarray(data_for_testing['emotion'], dtype='int32')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UfnNutH8eUAc","colab_type":"code","colab":{}},"source":["# Let's check the shape of every set\n","print('X_train = ',X_train.shape)\n","print('X_val = ', X_val.shape)\n","print('X_test = ', X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JCY0nYKOs8n0","colab_type":"text"},"source":["# **5. Parameters**"]},{"cell_type":"code","metadata":{"id":"G3zcqrLeuGoi","colab_type":"code","colab":{}},"source":["# -------- FOR MODEL ARCHITECTURE --------\n","number_classes = 7\n","number_features = 32\n","growthRate = 8\n","# -------- FOR MODEL ARCHITECTURE --------\n","\n","# -------- FOR LEARNING RATE SCHEDULE --------\n","learning_rate_decay_start = 80\n","learning_rate_decay_every = 5\n","learning_rate_decay_rate = 0.9\n","# -------- FOR LEARNING RATE SCHEDULE --------\n","\n","# -------- FOR FITTING THE MODEL --------\n","# NUMBER_OF_EPOCHS = 600\n","NUMBER_OF_EPOCHS = 1\n","batch_size = 64\n","num_samples = X_train.shape[0]\n","num_samples_val = X_val.shape[0]\n","steps = math.ceil(num_samples / batch_size)\n","steps_validation = math.ceil(num_samples_val / batch_size)\n","initial_learning_rate = 0.01\n","lambd = 0.0005\n","# -------- FOR FITTING THE MODEL --------\n","\n","# -------- OTHERS --------\n","filepath = \"/content/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n","loss_function = 'sparse_categorical_crossentropy'\n","regularizer = tf.keras.regularizers.l1(lambd)\n","verbosity = 1\n","# -------- OTHERS --------"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9QXEfiCqs8lM","colab_type":"text"},"source":["# **6. Learning rate schedule**"]},{"cell_type":"code","metadata":{"id":"N-5WqiSquHOH","colab_type":"code","colab":{}},"source":["# This function only get the parameters for learning rate scheduler\n","def get_setup_learning_rate_schedule():\n","  # for production code you should read them from the config file\n","  return (learning_rate_decay_start, learning_rate_decay_every, learning_rate_decay_rate, initial_learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdAfdYgugZ6L","colab_type":"code","colab":{}},"source":["def step_decay(epoch):\n","  learning_rate_decay_start, learning_rate_decay_every, learning_rate_decay_rate, initial_learning_rate = get_setup_learning_rate_schedule()\n","  if (epoch > learning_rate_decay_start):\n","    frac = (epoch - learning_rate_decay_start) // learning_rate_decay_every\n","    decay_factor = learning_rate_decay_rate ** frac\n","    initial_learning_rate = initial_learning_rate * decay_factor\n","    return initial_learning_rate\n","  else: return initial_learning_rate"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sm0oC6wcs8gF","colab_type":"text"},"source":["# **7. EdgeCNN implementation**"]},{"cell_type":"code","metadata":{"id":"yZcIix17uIDa","colab_type":"code","colab":{}},"source":["# Let's make the EdgeCNN arhictecture based on DenseNet, but with some additional changes in order to work well on Raspberry Pi 4\n","from tensorflow.keras import layers, models, Model\n","\n","# Create the function for Edge convolution block\n","# x: input layer \n","# growthRate: how many filter we want fot output\n","# @return: the layer after applying convolution block 1 and 2\n","def EdgeBlock(x, growthRate, name):\n","  # Convolution block 1\n","  x = layers.Conv2D(4 * growthRate, kernel_size = 3, kernel_regularizer = regularizer,padding = 'same', name='conv1_dense' + name)(x)\n","  x = layers.BatchNormalization(name='batchnorm1_dense' + name)(x)\n","  x = layers.ReLU(name='relu_dense'+name)(x)\n","\n","  # Convolution block 2\n","  x = layers.Conv2D(1 * growthRate, kernel_size = 3, kernel_regularizer = regularizer, padding = 'same', name='conv2_dense' + name)(x)\n","  x = layers.BatchNormalization(name='batchnorm2_dense' + name)(x)\n","  return x\n","\n","# Create the function for Dense Block\n","# input_layer: the layer on which we apply the EdgeBlock function of 'repetition' times \n","# growthRate: how many filter we want for output\n","# name: name of the denseblock densenumber_\n","# @return: concatenated layers after applying the EdgeBlock of 'repetition' times\n","def denseBlock(input_layer, repetition, growthRate, name):\n","  for i in range(repetition):\n","    # apply the convolution block 1 and 2\n","    x = EdgeBlock(input_layer, growthRate,str(name)+'_'+str(i))\n","    # concatenate with the input layer\n","    input_layer = layers.Concatenate()([input_layer,x])\n","  return input_layer\n","\n","# Create the transition layer\n","# input_layer: the layer on which we apply average pooling\n","# name: name of the layer\n","# @return: the layer with average pooling applied\n","def transitionLayer(input_layer, name):\n","  input_layer = layers.AveragePooling2D((2,2), strides = 2, name = 'transition_'+str(name))(input_layer)\n","  return input_layer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"652dyTbOheFS","colab_type":"code","colab":{}},"source":["input_shape = (48,48,1)\n","input_net = layers.Input(input_shape)\n","# First layer of convolution\n","x = layers.Conv2D(number_features,(3,3), kernel_regularizer = regularizer,padding = 'same', use_bias = True, strides = 1, name='conv0')(input_net)\n","x = layers.MaxPool2D((3,3),padding = 'same',strides = 2, name='pool0')(x)\n","\n","# Add the Dense layers\n","repetitions = 4, 4, 7\n","\n","layer_index = 1\n","for repetition in repetitions:\n","  dense_block = denseBlock(x, repetition, growthRate,name=layer_index)\n","  x = transitionLayer(dense_block, name=layer_index)\n","  layer_index+=1\n","\n","x = layers.GlobalAveragePooling2D(data_format='channels_last')(dense_block)\n","\n","output_layer = layers.Dense(number_classes, activation='softmax', kernel_regularizer = regularizer)(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"scordJ3_hfsS","colab_type":"code","colab":{}},"source":["EdgeCNN = Model(input_net, output_layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5whcdItxhlHn","colab_type":"code","colab":{}},"source":["EdgeCNN.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vl9cr-9_tl7g","colab_type":"text"},"source":["# **8. Callbacks & Online data augmentation**"]},{"cell_type":"code","metadata":{"id":"q9S4VTflQTWi","colab_type":"code","colab":{}},"source":["data_generator_train = ImageDataGenerator(\n","    rotation_range=10,\n","    horizontal_flip=True)\n","\n","data_generator_val = ImageDataGenerator(\n","    rotation_range=10,\n","    horizontal_flip=True)\n","\n","train_generator = data_generator_train.flow(X_train, y_train, batch_size=batch_size,seed=seed_value)\n","val_generator = data_generator_val.flow(X_val, y_val, batch_size=batch_size,seed=seed_value)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dq6YDwQuIo0","colab_type":"code","colab":{}},"source":["# Checkpoint callback\n","checkpoint_callback_sgd = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=2, save_best_only=True, mode='max')\n","\n","# Learning rate schedule callback\n","scheduler_lr = LearningRateScheduler(step_decay)\n","\n","#callbacks list\n","callbacks_list_sgd = [checkpoint_callback_sgd, scheduler_lr]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IL1-LBiZtl6e","colab_type":"text"},"source":["# **9. Compile**"]},{"cell_type":"code","metadata":{"id":"1Ru36v9suJaB","colab_type":"code","colab":{}},"source":["# SGD optimizer\n","sgd_optimizer = tf.optimizers.SGD(learning_rate=initial_learning_rate,  momentum=0.9)\n","\n","# Compile the model\n","EdgeCNN.compile(\n","    optimizer = sgd_optimizer,\n","    loss = loss_function,\n","    metrics = ['accuracy']\n",")\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KnumrGzntl3g","colab_type":"text"},"source":["# **10. Fit**"]},{"cell_type":"code","metadata":{"id":"GCBeSxtmuKBa","colab_type":"code","colab":{}},"source":["# Get the start time\n","start = time.time()\n","\n","# Let's fit the model\n","history = EdgeCNN.fit(\n","    train_generator,\n","    steps_per_epoch=steps,\n","    verbose = verbosity,\n","    epochs = NUMBER_OF_EPOCHS,\n","    validation_data = val_generator,\n","    validation_steps = steps_validation, \n","    callbacks=callbacks_list_sgd\n",")\n","# Get the finish time\n","end = time.time()\n","hours, rem = divmod(end-start, 3600)\n","minutes, seconds = divmod(rem, 60)\n","print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xu61JeYdtlzO","colab_type":"text"},"source":["# **11. Evaluate**"]},{"cell_type":"code","metadata":{"id":"6h_oJt4kuKd8","colab_type":"code","colab":{}},"source":["# Get the metrics\n","accuracy = history.history['accuracy']\n","val_accuracy = history.history['val_accuracy']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqGYbiPojLuL","colab_type":"code","colab":{}},"source":["# Get the max accuracy on the validation\n","max_accuracy = max(val_accuracy)\n","print('Max val accuracy: {:5.2f}%'.format(100*max_accuracy))\n","\n","# Get the epoch of max accuracy\n","at_epoch = np.asarray(val_accuracy, dtype='float32').argmax()\n","print('At epoch: ' + str(at_epoch + 1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJtVTpg-jfUK","colab_type":"code","colab":{}},"source":["def plot_metric(x,y1,y2,color1,color2,label1,label2, title):\n","  plt.subplots(figsize=(20,8)) \n","  plt.plot(x, y1, 'b', label=label1, color=color1)\n","  plt.plot(x, y2, 'b', label=label2, color=color2)\n","  plt.title(title)\n","  plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ptAOlCA6jp7Y","colab_type":"code","colab":{}},"source":["# Plot the accuracy on the training and validation set\n","epochs = range(len(accuracy))\n","sns.set(style=\"white\")\n","plot_metric(epochs,accuracy,val_accuracy, 'red','blue','training','validation', 'Training and valdiation ACCURACY')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"07_7rJ4Pj1nV","colab_type":"code","colab":{}},"source":["# Plot the loss on the training and validation set\n","epochs = range(len(accuracy))\n","plot_metric(epochs,loss,val_loss, 'red','blue','training','validation', 'Training and valdiation LOSS')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VHARf9MolRGV","colab_type":"code","colab":{}},"source":["# Get the accuracy and loss on test set\n","loss, acc = EdgeCNN.evaluate(X_test,  y_test,batch_size=batch_size, verbose=2)\n","print('Model, accuracy: {:5.2f}%'.format(100*acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8AsW6kOknfM","colab_type":"code","colab":{}},"source":["# Get the predictions of \n","y_pred = EdgeCNN.predict(X_test,batch_size=batch_size,verbose=verbosity)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ws4m_RuWk5iR","colab_type":"code","colab":{}},"source":["# Get the confusion matrix\n","conf_matrix = metrics.confusion_matrix(y_test, y_pred.argmax(axis = 1))\n","\n","# Plot the confusion matrix\n","figure = plt.figure(figsize=(8, 8))\n","sns.heatmap(conf_matrix, annot=True,cmap=plt.cm.Blues, fmt=\"d\")\n","plt.tight_layout()\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aIWL1a3ClB-G","colab_type":"code","colab":{}},"source":["# Print the classification Report\n","class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n","print(metrics.classification_report(y_test, y_pred.argmax(axis=1), target_names=class_names))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLIbsbLHle9U","colab_type":"code","colab":{}},"source":["# Get the ROC_AUC Score\n","metrics.roc_auc_score(y_test, y_pred,multi_class='ovr')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZXnUdVZEs8db","colab_type":"text"},"source":["# **12. Save**"]},{"cell_type":"code","metadata":{"id":"qK-qFu5buLiz","colab_type":"code","colab":{}},"source":["# Load the weights of the best checkpoint\n","# Set the path of the trained model\n","EdgeCNN.load_weights('/content/drive/My Drive/FaceX/tests/ReduceLROnPlateau/v0.0.1/weights-improvement-319-0.66.hdf5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlHqHOTwmAAK","colab_type":"code","colab":{}},"source":["# Save the model\n","# Set the path of where you want to save the model \n","EdgeCNN.save('/content/facex_tflite')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvVu1Uems8az","colab_type":"text"},"source":["# **13. Convert to TFLite** "]},{"cell_type":"code","metadata":{"id":"jy481rTNskIA","colab_type":"code","colab":{}},"source":["# Get the model from saved model file\n","# Set the path of where the model was saved\n","converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/FaceX/tflite/facex_tflite')\n","\n","# Convert the model\n","tflite_model = converter.convert()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VNyU7w3Wm8GP","colab_type":"code","colab":{}},"source":["# Write to file the Lite version of the model\n","# Set the path of where you want to save the model\n","open(\"/content/drive/My Drive/FaceX/tflite/facial_expression.tflite\", \"wb\").write(tflite_model)"],"execution_count":0,"outputs":[]}]}